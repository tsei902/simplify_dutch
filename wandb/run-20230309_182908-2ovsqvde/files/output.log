
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using custom data configuration default-8e0070c28d494a17
Found cached dataset text (C:/Users/Theresa/.cache/huggingface/datasets/text/default-8e0070c28d494a17/0.0.0/99cc88223027054f94ce0c7fd69d10eb172910fa0615671283a3c8e5e7af2f9c)
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 37.31it/s]
Using custom data configuration default-16388deb41941231
Found cached dataset text (C:/Users/Theresa/.cache/huggingface/datasets/text/default-16388deb41941231/0.0.0/99cc88223027054f94ce0c7fd69d10eb172910fa0615671283a3c8e5e7af2f9c)
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.27it/s]
Dataset({
    features: ['original', 'simple'],
    num_rows: 10
})
DatasetDict({
    train: Dataset({
        features: ['original', 'simple'],
        num_rows: 7
    })
    validation: Dataset({
        features: ['original', 'simple'],
        num_rows: 2
    })
    test: Dataset({
        features: ['original', 'simple'],
        num_rows: 1
    })
})
{'original': 'Toen Japan tien jaar later weer een race verdiende op het F1-schema, ging het in plaats daarvan naar Suzuka.', 'simple': 'Toen Japan tien jaar later weer werd toegevoegd aan het F1-schema, ging het in plaats daarvan naar Suzuka.'}
Traceback (most recent call last):
  File "c:\Users\Theresa\OneDrive - KU Leuven\Documents\GitHub\simplify_dutch\main.py", line 313, in <module>
    tokenized_datasets = dataset.map(preprocess_function_train(dataset['original'], dataset['simple']), batched=True)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\dataset_dict.py", line 57, in __getitem__
    return super().__getitem__(k)
KeyError: 'original'
[31mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [39m[1mTraceback (most recent call last)[31m[22m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[31mâ”‚[39m [33mc:\Users\Theresa\OneDrive - KU Leuven\Documents\GitHub\simplify_dutch\main.py[39m:[94m313[39m in [92m<module>[39m    [31mâ”‚
[31mâ”‚[39m                                                                                                  [31mâ”‚
[31mâ”‚[39m   310 â”‚   wandb.watch(model, log=[33m"all"[39m)                                                          [31mâ”‚
[31mâ”‚[39m   311 â”‚   dataset= get_data_txt(WIKILARGE_DATASET, [94m10[39m)                                           [31mâ”‚
[31mâ”‚[39m   312 â”‚   [96mprint[39m(dataset[[33m'train'[39m][[94m3[39m])                                                             [31mâ”‚
[31mâ”‚[39m [31mâ± [39m313 â”‚   tokenized_datasets = dataset.map(preprocess_function_train(dataset[[33m'original'[39m], data   [31mâ”‚
[31mâ”‚[39m   314 â”‚   [96mprint[39m(tokenized_datasets)                                                              [31mâ”‚
[31mâ”‚[39m   315 â”‚   time.sleep([94m7[39m)                                                                          [31mâ”‚
[31mâ”‚[39m   316 â”‚   tests= encoding_test(dataset)                                                          [31mâ”‚
[31mâ”‚[39m                                                                                                  [31mâ”‚
[31mâ”‚[39m [33mC:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\dataset_dict[39m [31mâ”‚
[31mâ”‚[39m [33m.py[39m:[94m57[39m in [92m__getitem__[39m                                                                            [31mâ”‚
[31mâ”‚[39m                                                                                                  [31mâ”‚
[31mâ”‚[39m     54 â”‚                                                                                         [31mâ”‚
[31mâ”‚[39m     55 â”‚   [94mdef[39m [92m__getitem__[39m([96mself[39m, k) -> Dataset:                                                  [31mâ”‚
[31mâ”‚[39m     56 â”‚   â”‚   [94mif[39m [96misinstance[39m(k, ([96mstr[39m, NamedSplit)) [95mor[39m [96mlen[39m([96mself[39m) == [94m0[39m:                            [31mâ”‚
[31mâ”‚[39m [31mâ± [39m  57 â”‚   â”‚   â”‚   [94mreturn[39m [96msuper[39m().[92m__getitem__[39m(k)                                                 [31mâ”‚
[31mâ”‚[39m     58 â”‚   â”‚   [94melse[39m:                                                                             [31mâ”‚
[31mâ”‚[39m     59 â”‚   â”‚   â”‚   available_suggested_splits = [                                                [31mâ”‚
[31mâ”‚[39m     60 â”‚   â”‚   â”‚   â”‚   split [94mfor[39m split [95min[39m (Split.TRAIN, Split.TEST, Split.VALIDATION) [94mif[39m split   [31mâ”‚
[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[1mKeyError: [32m[22m'original'