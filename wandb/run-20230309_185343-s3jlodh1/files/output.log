
  0%|                                                                                                | 0/3 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 1.0004, 'learning_rate': 0.0002, 'epoch': 1.0}
{'eval_loss': 3.8236207962036133, 'eval_runtime': 0.8746, 'eval_samples_per_second': 2.287, 'eval_steps_per_second': 1.143, 'epoch': 1.0}
 33%|█████████████████████████████▎                                                          | 1/3 [00:19<00:38, 19.11s/it]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: simple, original. If simple, original are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2
  Batch size = 8
 33%|█████████████████████████████▎                                                          | 1/3 [00:20<00:38, 19.11s/it]Saving model checkpoint to ./model_output/checkpoint-1
Configuration saved in ./model_output/checkpoint-1\config.json
Model weights saved in ./model_output/checkpoint-1\pytorch_model.bin
tokenizer config file saved in ./model_output/checkpoint-1\tokenizer_config.json
Special tokens file saved in ./model_output/checkpoint-1\special_tokens_map.json
{'loss': 0.9499, 'learning_rate': 0.0004, 'epoch': 2.0}
{'eval_loss': 3.6233508586883545, 'eval_runtime': 0.9031, 'eval_samples_per_second': 2.215, 'eval_steps_per_second': 1.107, 'epoch': 2.0}
 67%|██████████████████████████████████████████████████████████▋                             | 2/3 [00:41<00:20, 21.00s/it]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: simple, original. If simple, original are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2
  Batch size = 8
 67%|██████████████████████████████████████████████████████████▋                             | 2/3 [00:42<00:20, 21.00s/it]Saving model checkpoint to ./model_output/checkpoint-2
Configuration saved in ./model_output/checkpoint-2\config.json
Model weights saved in ./model_output/checkpoint-2\pytorch_model.bin
tokenizer config file saved in ./model_output/checkpoint-2\tokenizer_config.json
Special tokens file saved in ./model_output/checkpoint-2\special_tokens_map.json
{'loss': 0.6138, 'learning_rate': 0.0006, 'epoch': 3.0}
{'eval_loss': 3.514150619506836, 'eval_runtime': 0.9677, 'eval_samples_per_second': 2.067, 'eval_steps_per_second': 1.033, 'epoch': 3.0}
100%|████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:03<00:00, 21.69s/it]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: simple, original. If simple, original are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2
  Batch size = 8
100%|████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:04<00:00, 21.69s/it]Saving model checkpoint to ./model_output/checkpoint-3
Configuration saved in ./model_output/checkpoint-3\config.json
{'train_runtime': 84.1479, 'train_samples_per_second': 0.25, 'train_steps_per_second': 0.036, 'train_loss': 0.8547324339548746, 'epoch': 3.0}
Model weights saved in ./model_output/checkpoint-3\pytorch_model.bin
tokenizer config file saved in ./model_output/checkpoint-3\tokenizer_config.json
Special tokens file saved in ./model_output/checkpoint-3\special_tokens_map.json
Training completed. Do not forget to share your model on huggingface.co/models =)
Loading best model from ./model_output/checkpoint-3 (score: 3.514150619506836).
100%|████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:08<00:00, 22.90s/it]
Saving model checkpoint to ./saved_model
Configuration saved in ./saved_model\config.json
Model weights saved in ./saved_model\pytorch_model.bin
tokenizer config file saved in ./saved_model\tokenizer_config.json
Special tokens file saved in ./saved_model\special_tokens_map.json
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: simple, original. If simple, original are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 2
  Batch size = 8
100%|███████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 981.81it/s]
loading configuration file ./saved_model\config.json
Model config T5Config {
  "_name_or_path": "./saved_model",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 12,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.23.1",
  "use_cache": false,
  "vocab_size": 32103
}
loading weights file ./saved_model\pytorch_model.bin
./saved_model/training_args
All model checkpoint weights were used when initializing T5ForConditionalGeneration.
All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./saved_model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
Traceback (most recent call last):
  File "c:\Users\Theresa\OneDrive - KU Leuven\Documents\GitHub\simplify_dutch\main.py", line 331, in <module>
    for i in range(1,len(test_dataset['orig.dutch'])):
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\arrow_dataset.py", line 2356, in __getitem__
    return self._getitem(
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\arrow_dataset.py", line 2340, in _getitem
    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\formatting\formatting.py", line 460, in query_table
    _check_valid_column_key(key, table.column_names)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\formatting\formatting.py", line 400, in _check_valid_column_key
    raise KeyError(f"Column {key} not in the dataset. Current columns in the dataset: {columns}")
KeyError: "Column orig.dutch not in the dataset. Current columns in the dataset: ['asset.test.orig.dutch', 'asset.test.simp.0.dutch', 'asset.test.simp.1.dutch', 'asset.test.simp.2.dutch', 'asset.test.simp.3.dutch', 'asset.test.simp.4.dutch', 'asset.test.simp.5.dutch']"
[31m╭─────────────────────────────── [39m[1mTraceback (most recent call last)[31m[22m ────────────────────────────────╮
[31m│[39m [33mc:\Users\Theresa\OneDrive - KU Leuven\Documents\GitHub\simplify_dutch\main.py[39m:[94m331[39m in [92m<module>[39m    [31m│
[31m│[39m                                                                                                  [31m│
[31m│[39m   328 │   # generated_dataset= generate(tokenized_test_input['input_ids'], trained_model, toke   [31m│
[31m│[39m   329 │   # print(generated_dataset)                                                             [31m│
[31m│[39m   330 │                                                                                          [31m│
[31m│[39m [31m❱ [39m331 │   [94mfor[39m i [95min[39m [96mrange[39m([94m1[39m,[96mlen[39m(test_dataset[[33m'orig.dutch'[39m])):                                     [31m│
[31m│[39m   332 │   │                                                                                      [31m│
[31m│[39m   333 │   │   tokenized_test_input = preprocess_function_test(test_dataset[[33m'orig.dutch'[39m][i])     [31m│
[31m│[39m   334 │   │   generated_dataset= generate(tokenized_test_input[[33m'input_ids'[39m], trained_model, to   [31m│
[31m│[39m                                                                                                  [31m│
[31m│[39m [33mC:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\arrow_datase[39m [31m│
[31m│[39m [33mt.py[39m:[94m2356[39m in [92m__getitem__[39m                                                                         [31m│
[31m│[39m                                                                                                  [31m│
[31m│[39m   2353 │                                                                                         [31m│
[31m│[39m   2354 │   [94mdef[39m [92m__getitem__[39m([96mself[39m, key):  # noqa: F811                                             [31m│
[31m│[39m   2355 │   │   [33m"""Can be used to index columns (by string names) or rows (by integer index or i[39m  [31m│
[31m│[39m [31m❱ [39m2356 │   │   [94mreturn[39m [96mself[39m._getitem(                                                             [31m│
[31m│[39m   2357 │   │   │   key,                                                                          [31m│
[31m│[39m   2358 │   │   )                                                                                 [31m│
[31m│[39m   2359                                                                                           [31m│
[31m│[39m                                                                                                  [31m│
[31m│[39m [33mC:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\arrow_datase[39m [31m│
[31m│[39m [33mt.py[39m:[94m2340[39m in [92m_getitem[39m                                                                            [31m│
[31m│[39m                                                                                                  [31m│
[31m│[39m   2337 │   │   format_kwargs = kwargs[[33m"format_kwargs"[39m] [94mif[39m [33m"format_kwargs"[39m [95min[39m kwargs [94melse[39m [96mself[39m._  [31m│
[31m│[39m   2338 │   │   format_kwargs = format_kwargs [94mif[39m format_kwargs [95mis[39m [95mnot[39m [94mNone[39m [94melse[39m {}                [31m│
[31m│[39m   2339 │   │   formatter = get_formatter(format_type, features=[96mself[39m.features, decoded=decoded,   [31m│
[31m│[39m [31m❱ [39m2340 │   │   pa_subtable = query_table([96mself[39m._data, key, indices=[96mself[39m._indices [94mif[39m [96mself[39m._indice  [31m│
[31m│[39m   2341 │   │   formatted_output = format_table(                                                  [31m│
[31m│[39m   2342 │   │   │   pa_subtable, key, formatter=formatter, format_columns=format_columns, output  [31m│
[31m│[39m   2343 │   │   )                                                                                 [31m│
[31m│[39m                                                                                                  [31m│
[31m│[39m [33mC:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\formatting\f[39m [31m│
[31m│[39m [33mormatting.py[39m:[94m460[39m in [92mquery_table[39m                                                                  [31m│
[31m│[39m                                                                                                  [31m│
[31m│[39m   457 │   [94mif[39m [95mnot[39m [96misinstance[39m(key, ([96mint[39m, [96mslice[39m, [96mrange[39m, [96mstr[39m, Iterable)):                            [31m│
[31m│[39m   458 │   │   _raise_bad_key_type(key)                                                           [31m│
[31m│[39m   459 │   [94mif[39m [96misinstance[39m(key, [96mstr[39m):                                                               [31m│
[31m│[39m [31m❱ [39m460 │   │   _check_valid_column_key(key, table.column_names)                                   [31m│
[31m│[39m   461 │   [94melse[39m:                                                                                  [31m│
[31m│[39m   462 │   │   size = indices.num_rows [94mif[39m indices [95mis[39m [95mnot[39m [94mNone[39m [94melse[39m table.num_rows                 [31m│
[31m│[39m   463 │   │   _check_valid_index_key(key, size)                                                  [31m│
[31m│[39m                                                                                                  [31m│
[31m│[39m [33mC:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\formatting\f[39m [31m│
[31m│[39m [33mormatting.py[39m:[94m400[39m in [92m_check_valid_column_key[39m                                                      [31m│
[31m│[39m                                                                                                  [31m│
[31m│[39m   397                                                                                            [31m│
[31m│[39m   398 [94mdef[39m [92m_check_valid_column_key[39m(key: [96mstr[39m, columns: List[[96mstr[39m]) -> [94mNone[39m:                         [31m│
[31m│[39m   399 │   [94mif[39m key [95mnot[39m [95min[39m columns:                                                                 [31m│
[31m│[39m [31m❱ [39m400 │   │   [94mraise[39m [96mKeyError[39m([33mf"Column {[39mkey[33m} not in the dataset. Current columns in the dataset[39m   [31m│
[31m│[39m   401                                                                                            [31m│
[31m│[39m   402                                                                                            [31m│
[31m│[39m   403 [94mdef[39m [92m_check_valid_index_key[39m(key: Union[[96mint[39m, [96mslice[39m, [96mrange[39m, Iterable], size: [96mint[39m) -> [94mNone[39m:    [31m│
[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
[1mKeyError: [32m[22m"Column orig.dutch not in the dataset. Current columns in the dataset: ['asset.test.orig.dutch', 
[32m'asset.test.simp.0.dutch', 'asset.test.simp.1.dutch', 'asset.test.simp.2.dutch', 'asset.test.simp.3.dutch', 
[32m'asset.test.simp.4.dutch', 'asset.test.simp.5.dutch']"