
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using custom data configuration default-bdb956b97b5c010b
Found cached dataset text (C:/Users/Theresa/.cache/huggingface/datasets/text/default-bdb956b97b5c010b/0.0.0/99cc88223027054f94ce0c7fd69d10eb172910fa0615671283a3c8e5e7af2f9c)
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1001.51it/s]
Using custom data configuration default-c45b328b6e81bd65
Found cached dataset text (C:/Users/Theresa/.cache/huggingface/datasets/text/default-c45b328b6e81bd65/0.0.0/99cc88223027054f94ce0c7fd69d10eb172910fa0615671283a3c8e5e7af2f9c)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 974.51it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.51ba/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 147.71ba/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 250.15ba/s]
DatasetDict({
    train: Dataset({
        features: ['original'],
        num_rows: 2000
    })
})
DatasetDict({
    train: Dataset({
        features: ['simple'],
        num_rows: 2000
    })
})
Dataset({
    features: ['original', 'simple'],
    num_rows: 50
})
DatasetDict({
    train: Dataset({
        features: ['original', 'simple'],
        num_rows: 35
    })
    validation: Dataset({
        features: ['original', 'simple'],
        num_rows: 10
    })
    test: Dataset({
        features: ['original', 'simple'],
        num_rows: 5
    })
})
DatasetDict({
    train: Dataset({
        features: ['original', 'simple', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 35
    })
    validation: Dataset({
        features: ['original', 'simple', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 10
    })
    test: Dataset({
        features: ['original', 'simple', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 5
    })
})
sentence 1
input_sentence:  Een jachthond verwijst naar elke hond die mensen helpt bij de jacht.</s>
labels:  Een jachthond helpt mensen tijdens het jagen.</s>
sentence 2
input_sentence:  De depressie volgde in noordwestelijke richting en trok vroeg op 22 september aan land bij Fort Walton Beach, en kort daarna verdween hij boven het zuidoosten van Alabama.</s>
labels:  De storm trok vroeg op 22 september aan land bij Fort Walton Beach. Daarna verdween hij boven het zuidoosten van Alabama.</s>
sentence 3
input_sentence:  Landbouw De lokale bevolking verbouwt seizoensgewassen zoals maïs (makai) en tarwe (kanak).</s>
labels:  De lokale bevolking verbouwt seizoensgewassen zoals maïs (makai) en tarwe (kanak).</s>
sentence 4
input_sentence:  De auteur(s) en uitgever(s) van het Document geven door deze Licentie geen toestemming om hun naam te gebruiken voor publiciteit voor of om goedkeuring van enige Gewijzigde Versie te bevestigen of te impliceren.</s>
labels:  De auteur of uitgever van het document geeft geen toestemming om hun naam te gebruiken. Dit geldt ook voor publiciteit of goedkeuring van een gewijzigde versie.</s>
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: simple, original. If simple, original are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
***** Running training *****
  Num examples = 35
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 3
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|                                                                                                                                                                                                        | 0/3 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 33%|████████████████████████████████████████████████████████████████                                                                                                                                | 1/3 [00:37<01:14, 37.30s/it]Traceback (most recent call last):
  File "c:\Users\Theresa\OneDrive - KU Leuven\Documents\GitHub\simplify_dutch\main.py", line 260, in <module>
    trainer.train()
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\trainer.py", line 1500, in train
    return inner_training_loop(
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\trainer.py", line 1742, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\trainer.py", line 2504, in training_step
    loss.backward()
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\autograd\__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\wandb\wandb_torch.py", line 282, in <lambda>
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
KeyboardInterrupt
Error in sys.excepthook:
Traceback (most recent call last):
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\console.py", line 1694, in print
    extend(render(renderable, render_options))
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\console.py", line 1330, in render
    yield from self.render(render_output, _options)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\console.py", line 1326, in render
    for render_output in iter_render:
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\constrain.py", line 29, in __rich_console__
    yield from console.render(self.renderable, child_options)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\console.py", line 1326, in render
    for render_output in iter_render:
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\panel.py", line 220, in __rich_console__
    lines = console.render_lines(renderable, child_options, style=style)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\console.py", line 1366, in render_lines
    lines = list(
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\console.py", line 1326, in render
    for render_output in iter_render:
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\padding.py", line 97, in __rich_console__
    lines = console.render_lines(
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\console.py", line 1366, in render_lines
    lines = list(
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\console.py", line 1330, in render
    yield from self.render(render_output, _options)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\console.py", line 1326, in render
    for render_output in iter_render:
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\syntax.py", line 609, in __rich_console__
    segments = Segments(self._get_syntax(console, options))
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\segment.py", line 668, in __init__
    self.segments = list(segments)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\syntax.py", line 637, in _get_syntax
    text = self.highlight(processed_code, self.line_range)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\syntax.py", line 470, in highlight
    lexer = self.lexer
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\rich\syntax.py", line 433, in lexer
    return get_lexer_by_name(
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\pygments\lexers\__init__.py", line 107, in get_lexer_by_name
    return _lexer_cache[name](**options)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\pygments\lexer.py", line 580, in __call__
    cls._tokens = cls.process_tokendef('', cls.get_tokendefs())
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\pygments\lexer.py", line 519, in process_tokendef
    cls._process_state(tokendefs, processed, state)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\pygments\lexer.py", line 482, in _process_state
    tokens.extend(cls._process_state(unprocessed, processed,
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\pygments\lexer.py", line 482, in _process_state
    tokens.extend(cls._process_state(unprocessed, processed,
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\pygments\lexer.py", line 498, in _process_state
    rex = cls._process_regex(tdef[0], rflags, state)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\pygments\lexer.py", line 427, in _process_regex
    return re.compile(regex, rflags).match
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\re.py", line 251, in compile
    return _compile(pattern, flags)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\re.py", line 303, in _compile
    p = sre_compile.compile(pattern, flags)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\sre_compile.py", line 792, in compile
    code = _code(p, flags)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\sre_compile.py", line 628, in _code
    _compile_info(code, p, flags)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\sre_compile.py", line 614, in _compile_info
    charset, hascased = _optimize_charset(charset)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\sre_compile.py", line 328, in _optimize_charset
    charmap[i] = 1
KeyboardInterrupt
Original exception was:
Traceback (most recent call last):
  File "c:\Users\Theresa\OneDrive - KU Leuven\Documents\GitHub\simplify_dutch\main.py", line 260, in <module>
    trainer.train()
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\trainer.py", line 1500, in train
    return inner_training_loop(
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\trainer.py", line 1742, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\trainer.py", line 2504, in training_step
    loss.backward()
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\autograd\__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "C:\Users\Theresa\AppData\Local\Programs\Python\Python310\lib\site-packages\wandb\wandb_torch.py", line 282, in <lambda>
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
KeyboardInterrupt