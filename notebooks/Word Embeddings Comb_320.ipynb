{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import re\n",
    "from sacremoses import MosesTokenizer\n",
    "import Levenshtein\n",
    "import spacy\n",
    "import nltk\n",
    "import pickle\n",
    "import urllib\n",
    "import os\n",
    "import tarfile\n",
    "import zipfile\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "# import paths\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "RESOURCES_DIR = Path('../resources')\n",
    "DATASETS_PATH = RESOURCES_DIR / \"datasets\"\n",
    "WORD_EMBEDDINGS_NAME ='combined_320'\n",
    "DUMPS_DIR = RESOURCES_DIR / \"DUMPS\"\n",
    "\n",
    "import gensim\n",
    "stopwords = set(stopwords.words(\"dutch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ControlDivisionByZero(numerator, denominator):\n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "\n",
    "class FeatureAbstract(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_ratio(self, kwargs):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def calculate_ratio(self, simple_text, original_text):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Feature(FeatureAbstract):\n",
    "\n",
    "    def __init__(self, split, target_ratio):\n",
    "        self.split = split\n",
    "        self.target_ratio = target_ratio\n",
    "\n",
    "    def get_ratio(self, kwargs):\n",
    "        if not 'original_text_preprocessed' in kwargs:\n",
    "            kwargs['original_text_preprocessed'] = \"\"\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            simple_text = kwargs.get('simple_text')\n",
    "            original_text = kwargs.get('original_text')\n",
    "            result_ratio = self.calculate_ratio(simple_text, original_text)\n",
    "\n",
    "        elif self.split == \"valid\" or self.split == \"test\":\n",
    "            result_ratio = self.target_ratio\n",
    "        else:\n",
    "            raise ValueError(\"stage value not supported\")\n",
    "        kwargs['original_text_preprocessed'] += f'{self.name}_{result_ratio} '\n",
    "        return kwargs\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        name = \"\"\n",
    "        for word in re.findall('[A-Z][^A-Z]*', class_name):\n",
    "            if word: name += word[0]\n",
    "        if not name: name = class_name\n",
    "        return name\n",
    "\n",
    "\n",
    "class WordLengthRatio(Feature):\n",
    "\n",
    "    def __init__(self, stage, target_ratio):\n",
    "        super().__init__(stage, target_ratio)\n",
    "        if stage == \"train\":\n",
    "            # THIS IS A WORD TOKENIZER, we need one for dutch\n",
    "            # nl_core_news_sm spacy  spacy.nl_core_news_sm \n",
    "            # nltk.word_tokenize\n",
    "            self.tokenizer =  MosesTokenizer(lang='nl') #  nltk.word_tokenize(language='dutch')  # Moses Tokenizer for Dutch language\n",
    "\n",
    "    def calculate_ratio(self, simple_text, original_text):\n",
    "        return round(ControlDivisionByZero(\n",
    "            len(self.tokenizer.tokenize(simple_text)),\n",
    "            len(self.tokenizer.tokenize(original_text))), 2)\n",
    "\n",
    "\n",
    "class CharLengthRatio(Feature):\n",
    "\n",
    "    def __init__(self, stage, target_ratio):\n",
    "        super().__init__(stage, target_ratio)\n",
    "\n",
    "    def calculate_ratio(self, simple_text, original_text):\n",
    "        return round(ControlDivisionByZero(len(simple_text),\n",
    "            len(original_text)), 2)\n",
    "\n",
    "\n",
    "class LevenshteinRatio(Feature):\n",
    "\n",
    "    def __init__(self, stage, target_ratio):\n",
    "        super().__init__(stage, target_ratio)\n",
    "\n",
    "    def calculate_ratio(self, simple_text, original_text):\n",
    "        simple_text = word_tokenize(simple_text,language='dutch')\n",
    "        original_text = word_tokenize(original_text,language='dutch')\n",
    "        # complex_sentence = tokenize(complex_sentence) # ,language='dutch')\n",
    "\n",
    "        # simple_sentence = tokenize(simple_sentence) # ,language='dutch')\n",
    "        return round(Levenshtein.seqratio(original_text,\n",
    "                                       simple_text), 2)\n",
    "\n",
    "\n",
    "class DependencyTreeDepthRatio(Feature):\n",
    "\n",
    "    def __init__(self, stage, target_ratio):\n",
    "        super().__init__(stage, target_ratio)\n",
    "        if stage == \"train\":\n",
    "            self.nlp = self.get_spacy_model()\n",
    "\n",
    "    def get_spacy_model(self):\n",
    "\n",
    "        model = 'nl_core_news_sm'  # from spacy, Dutch pipeline optimized for CPU. Components: tok2vec, morphologizer, tagger, parser, lemmatizer (trainable_lemmatizer), senter, ner.\n",
    "        if not spacy.util.is_package(model):\n",
    "            spacy.cli.download(model)\n",
    "            spacy.cli.link(model, model, force=True, model_path=spacy.util.get_package_path(model))\n",
    "        return spacy.load(model)\n",
    "\n",
    "    def calculate_ratio(self, simple_text, original_text):\n",
    "\n",
    "        result_ratio = round(ControlDivisionByZero(\n",
    "            self.get_dependency_tree_depth(simple_text),\n",
    "            self.get_dependency_tree_depth(original_text)), 2)\n",
    "\n",
    "        return result_ratio\n",
    "\n",
    "    def get_dependency_tree_depth(self, sentence):\n",
    "\n",
    "        def get_subtree_depth(node):\n",
    "            if len(list(node.children)) == 0:\n",
    "                return 0\n",
    "            return 1 + max([get_subtree_depth(child) for child in node.children])\n",
    "\n",
    "        tree_depths = [get_subtree_depth(spacy_sentence.root) for spacy_sentence in self.nlp(sentence).sents]\n",
    "        if len(tree_depths) == 0:\n",
    "            return 0\n",
    "        return max(tree_depths)\n",
    "\n",
    "\n",
    "class WordRankRatio(Feature):\n",
    "    # single underscore = internally\n",
    "\n",
    "    def __init__(self, stage, target_ratio): # constructor of the class \n",
    "        super().__init__(stage, target_ratio)\n",
    "        if stage == \"train\":\n",
    "            self.tokenizer = MosesTokenizer(lang='nl')\n",
    "            self.word2rank = self._get_word2rank()\n",
    "            print('finished get word2rank')\n",
    "            # store in file\n",
    "            # if not present, make file\n",
    "            # with open (\"./resources/DUMPS/word2rank.txt\", \"a\", encoding=\"utf8\") as file: \n",
    "            #     file.writelines(self.word2rank)\n",
    "            #     file.write(\"\\n\")\n",
    "            #     file.close()\n",
    "            print('length of word2rank', len(self.word2rank))\n",
    "            self.length_rank = len(self.word2rank) # hier length of the file! \n",
    "\n",
    "    def calculate_ratio(self, simple_text, original_text):\n",
    "\n",
    "        result_ratio = round(min(ControlDivisionByZero(self.get_lexical_complexity_score(simple_text),\n",
    "                                                       self.get_lexical_complexity_score(original_text)),\n",
    "                                 2), 2)\n",
    "\n",
    "        return result_ratio\n",
    "\n",
    "    def get_lexical_complexity_score(self, sentence, quantile_value=0.75):\n",
    "\n",
    "        words = self.tokenizer.tokenize(self._remove_stopwords(self._remove_punctuation(sentence)))\n",
    "        words = [word for word in words if word in self.word2rank]\n",
    "        if len(words) == 0:\n",
    "            return np.log(1 + self.length_rank)\n",
    "        return np.quantile([self._get_rank(word) for word in words], quantile_value)\n",
    "\n",
    "    def _remove_punctuation(self, text):\n",
    "        return ' '.join([word for word in self.tokenizer.tokenize(text) if not self._is_punctuation(word)])\n",
    "\n",
    "    def _remove_stopwords(self, text):\n",
    "        return ' '.join([w for w in self.tokenizer.tokenize(text) if w.lower() not in stopwords])\n",
    "\n",
    "    def _is_punctuation(self, word):\n",
    "        return ''.join([char for char in word if char not in punctuation]) == ''\n",
    "\n",
    "    def _get_rank(self, word):\n",
    "        rank = self.word2rank.get(word, self.length_rank)\n",
    "        return np.log(1 + rank)\n",
    "\n",
    "    def _get_word2rank(self, vocab_size=np.inf):\n",
    "        model_filepath = DUMPS_DIR / f\"{WORD_EMBEDDINGS_NAME}.pk\"\n",
    "        if model_filepath.exists():\n",
    "            with open(model_filepath, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            return model\n",
    "        else:\n",
    "            print(\"Opening conll model...\")\n",
    "            print(\"Preprocessing word2rank...\")\n",
    "            DUMPS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "            WORD_EMBEDDINGS_PATH = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "            lines_generator = self._yield_lines(WORD_EMBEDDINGS_PATH)\n",
    "            word2rank = {}\n",
    "            # next(lines_generator)\n",
    "            for i, line in enumerate(lines_generator):\n",
    "                if i >= vocab_size: break\n",
    "                word = line.split(' ')[0]\n",
    "                word2rank[word] = i\n",
    "\n",
    "            pickle.dump(word2rank, open(model_filepath, 'wb'))\n",
    "            # txt_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "            # zip_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.zip'\n",
    "            # if txt_file.exists(): txt_file.unlink()\n",
    "            # if zip_file.exists(): zip_file.unlink()\n",
    "            return word2rank\n",
    "        \n",
    "        # else:            \n",
    "        #     print(\"Downloading dutch embeddings ...\") # pretrained vectors\n",
    "        #     self._download_twitter_embeddings(model_name='coostco', dest_dir=str(DUMPS_DIR))\n",
    "        #     print(\"Preprocessing word2rank...\")\n",
    "        #     DUMPS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        #     WORD_EMBEDDINGS_PATH = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.bin'\n",
    "        #     model = self._load_word_embeddings(WORD_EMBEDDINGS_PATH) # returns index_to_key\n",
    "        #     # store into file\n",
    "        #     lines_generator = model # self._yield_lines(model) # (WORD_EMBEDDINGS_PATH)\n",
    "            \n",
    "        #     word2rank = {}\n",
    "        #     # next(lines_generator)\n",
    "        #     print('vocab_size', vocab_size)\n",
    "        #     for i, line in enumerate(lines_generator):\n",
    "        #         if i >= vocab_size: break # its not vocab size any more but  # len(model.key_to_index)\n",
    "        #         word = line.split(',')[0]\n",
    "        #         print('word', word)\n",
    "        #         word2rank[word] = i\n",
    "        #         print('ranked word?', word2rank[word])\n",
    "                \n",
    "        #     pickle.dump(word2rank, open(model_filepath, 'wb'))\n",
    "        #     txt_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "        #     zip_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.zip'\n",
    "        #     # if txt_file.exists(): txt_file.unlink()\n",
    "        #     # if zip_file.exists(): zip_file.unlink()\n",
    "        #     # print(word2rank)\n",
    "        #     return word2rank\n",
    "    \n",
    "    def _load_word_embeddings(self, filepath):\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(filepath, binary=True) # '../resources/DUMPS/model.bin'\n",
    "        model_indexes = model.index_to_key\n",
    "        return model_indexes\n",
    "    \n",
    "    def _download_twitter_embeddings(self, model_name, dest_dir): # pretrained rankings\n",
    "        url = ''\n",
    "        if model_name == 'coosto_model':\n",
    "            url = 'https://github.com/coosto/dutch-word-embeddings/releases/download/v1.0/model.bin'\n",
    "        file_path = self._download_url(url, dest_dir)\n",
    "        out_filepath = Path(file_path)\n",
    "        out_filepath = out_filepath.parent / f'{out_filepath.stem}.txt'\n",
    "        # print(out_filepath, out_filepath.exists())\n",
    "        if not out_filepath.exists():\n",
    "            print(\"Extracting: \", Path(file_path).name)\n",
    "            self._unzip(file_path, dest_dir)\n",
    "\n",
    "    def _yield_lines(self, filepath):\n",
    "        filepath = Path(filepath)\n",
    "        with filepath.open('r', encoding=\"latin-1\") as f:\n",
    "            for line in f:\n",
    "                # print(line)\n",
    "                yield line.rstrip()\n",
    "\n",
    "    def _download_url(self, url, output_path):\n",
    "        name = url.split('/')[-1]\n",
    "        file_path = f'{output_path}/{name}'\n",
    "        if not Path(file_path).exists():\n",
    "            with tqdm(unit='B', unit_scale=True, leave=True, miniters=1,\n",
    "                      desc=name) as t:  # all optional kwargs\n",
    "                urllib.request.urlretrieve(url, filename=file_path, reporthook=self._download_report_hook(t), data=None)\n",
    "        return file_path\n",
    "\n",
    "    def _unzip(self, file_path, dest_dir=None):\n",
    "        if dest_dir is None:\n",
    "            dest_dir = os.path.dirname(file_path)\n",
    "        if file_path.endswith('.zip'):\n",
    "            with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(dest_dir)\n",
    "        elif file_path.endswith(\"tar.gz\") or file_path.endswith(\"tgz\"):\n",
    "            tar = tarfile.open(file_path, \"r:gz\")\n",
    "            tar.extractall(dest_dir)\n",
    "            tar.close()\n",
    "        elif file_path.endswith(\"tar\"):\n",
    "            tar = tarfile.open(file_path, \"r:\")\n",
    "            tar.extractall(dest_dir)\n",
    "            tar.close()\n",
    "\n",
    "    def _download_report_hook(self, t):\n",
    "        last_b = [0]\n",
    "\n",
    "        def inner(b=1, bsize=1, tsize=None):\n",
    "            if tsize is not None:\n",
    "                t.total = tsize\n",
    "            t.update((b - last_b[0]) * bsize)\n",
    "            last_b[0] = b\n",
    "\n",
    "        return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_sentence = \"Sommige steden aan de Eyre Highway in de zuidoostelijke hoek van West-Australië, tussen de Zuid-Australische grens bijna tot aan Caiguna, volgen de officiële West-Australische tijd niet.\"\n",
    "simple_sentence = \"Sommige steden in West-Australië gebruiken geen West-Australische tijd.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished get word2rank\n",
      "length of word2rank 989820\n"
     ]
    }
   ],
   "source": [
    "wordRank = WordRankRatio(\"train\", 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.9917687012441645"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordRank.get_lexical_complexity_score(complex_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.579682160499392"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordRank.get_lexical_complexity_score(simple_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_sentence = \"Het ruimtevaartuig bestaat uit twee hoofdelementen: de NASA Cassini-orbiter, genoemd naar de Italiaans-Franse astronoom Giovanni Domenico Cassini, en de ESA Huygens-sonde, genoemd naar de Nederlandse astronoom, wiskundige en natuurkundige Christiaan Huygens.\"\n",
    "simple_sentence = \"Het ruimtevaartuig heeft twee hoofdelementen: de NASA Cassini-orbiter en de ESA Huygens-sonde.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.972449192984996"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordRank.get_lexical_complexity_score(complex_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.278365319538235"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordRank.get_lexical_complexity_score(simple_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_sentence = \"Dit was het gebied ten oosten van de monding van de rivier de Vistula, later ook wel 'eigenlijk Pruisen' genoemd.\"\n",
    "simple_sentence=\"Pruisen was eigenlijk de plaats ten oosten van de monding van de rivier de Vistula.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.927081784052815"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordRank.get_lexical_complexity_score(complex_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.842432913359531"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordRank.get_lexical_complexity_score(simple_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('../resources/DUMPS/combined_320.txt', binary=False, limit=980000, encoding='utf8') # 989820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('van', 0.6756626963615417),\n",
       " ('in', 0.6316277384757996),\n",
       " ('een', 0.6282400488853455),\n",
       " ('en', 0.5985819101333618),\n",
       " ('eveneens', 0.5922880172729492),\n",
       " ('honderddagenoffensief', 0.5661090612411499),\n",
       " ('die', 0.565862238407135),\n",
       " ('maslenica', 0.562157928943634),\n",
       " ('tourseizoen', 0.5596410036087036),\n",
       " ('andreaspenning', 0.5588655471801758)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rivier', 0.8399328589439392),\n",
       " ('benedenloop', 0.8128615021705627),\n",
       " ('bovenloop', 0.7880266308784485),\n",
       " ('estuarium', 0.7785959243774414),\n",
       " ('samenvloeiing', 0.778411865234375),\n",
       " ('uitmonding', 0.7749341726303101),\n",
       " ('stroomafwaarts', 0.7470329403877258),\n",
       " ('pjasina', 0.7449982166290283),\n",
       " ('stroomopwaarts', 0.7386379837989807),\n",
       " ('baai', 0.7342506051063538)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"monding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eiland', 0.7068321704864502),\n",
       " ('koerileneiland', 0.7001560926437378),\n",
       " ('tsjoektsjenschiereiland', 0.6942179799079895),\n",
       " ('iberisch', 0.6905044913291931),\n",
       " ('vasteland', 0.6888741254806519),\n",
       " ('reykjanes', 0.6877791285514832),\n",
       " ('noordkust', 0.6849595904350281),\n",
       " ('zuidkust', 0.6840841174125671),\n",
       " ('westelijkste', 0.6813684105873108),\n",
       " ('snæfellsnes', 0.680156409740448)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"schiereiland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('keratine', 0.8063088059425354),\n",
       " ('exoskelet', 0.7815277576446533),\n",
       " ('peptidoglycaan', 0.777212917804718),\n",
       " ('polysachariden', 0.7538168430328369),\n",
       " ('suberine', 0.7455270290374756),\n",
       " ('polysacharide', 0.7441681027412415),\n",
       " ('conchyoline', 0.7357071042060852),\n",
       " ('hemicellulose', 0.7300746440887451),\n",
       " ('celwanden', 0.7300340533256531),\n",
       " ('bètakeratine', 0.7290903329849243)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"chitine\") # what to do with that? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('muiters', 0.6793466806411743),\n",
       " ('sepoy', 0.6448593735694885),\n",
       " ('hmav', 0.6437531113624573),\n",
       " ('oproer', 0.6362000107765198),\n",
       " ('opstand', 0.6251309514045715),\n",
       " ('vlieterincident', 0.6169967651367188),\n",
       " ('rebellie', 0.6056457757949829),\n",
       " ('scheepsmacht', 0.6023922562599182),\n",
       " ('mantotmangevechten', 0.6016296148300171),\n",
       " ('torpedering', 0.6006870269775391)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"muiterij\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pomaken', 0.7338395118713379),\n",
       " ('turkomannen', 0.6837456822395325),\n",
       " ('beloetsjen', 0.6833829283714294),\n",
       " ('gorani', 0.6724549531936646),\n",
       " ('arbëreshë', 0.672112762928009),\n",
       " ('aroemenen', 0.666069746017456),\n",
       " ('vlachen', 0.6630237102508545),\n",
       " ('gagaoezen', 0.6590104103088379),\n",
       " ('janjevci', 0.6576855778694153),\n",
       " ('pomakken', 0.6495280861854553)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"moslimminderheid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bovenverdieping', 0.8295695781707764),\n",
       " ('beletage', 0.763651430606842),\n",
       " ('verdieping', 0.7369603514671326),\n",
       " ('bovenverdiepingen', 0.7291617393493652),\n",
       " ('oostvleugel', 0.7169917821884155),\n",
       " ('binnenkoer', 0.7102579474449158),\n",
       " ('dienstruimten', 0.6976855397224426),\n",
       " ('hoofdverdieping', 0.6954277157783508),\n",
       " ('begane', 0.6929664015769958),\n",
       " ('dienstruimte', 0.692493736743927)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"benedenverdieping\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chirurgische', 0.7050020098686218),\n",
       " ('medische', 0.6709998846054077),\n",
       " ('orthodontische', 0.64791339635849),\n",
       " ('trombolyse', 0.6358797550201416),\n",
       " ('orthopedische', 0.6326757669448853),\n",
       " ('fysiotherapeutische', 0.6326483488082886),\n",
       " ('interventionele', 0.6310393810272217),\n",
       " ('minimaalinvasieve', 0.6308104395866394),\n",
       " ('anesthesioloog', 0.6302860379219055),\n",
       " ('medischspecialistische', 0.6259549260139465)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"tandheelkundige\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16965"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.key_to_index[\"impasse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gitaarsnaar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mkey_to_index[\u001b[39m\"\u001b[39;49m\u001b[39mgitaarsnaar\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gitaarsnaar'"
     ]
    }
   ],
   "source": [
    "model.key_to_index[\"gitaarsnaar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154282"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.key_to_index[\"aardnoten\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.key_to_index[\"van\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Warmtekrachtkoppeling' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mmost_similar(\u001b[39m\"\u001b[39;49m\u001b[39mWarmtekrachtkoppeling\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Theresa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[39m=\u001b[39m item[\u001b[39m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[39m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_mean_vector(keys, weight, pre_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, post_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, ignore_missing\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    842\u001b[0m all_keys \u001b[39m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_index(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, _KEY_TYPES) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[39mif\u001b[39;00m indexer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(topn, \u001b[39mint\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Theresa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present in vocabulary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[39mif\u001b[39;00m total_weight \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[39m=\u001b[39m mean \u001b[39m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'Warmtekrachtkoppeling' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.most_similar(\"Warmtekrachtkoppeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de', 0.6756627559661865),\n",
       " ('en', 0.63153475522995),\n",
       " ('in', 0.6282373070716858),\n",
       " ('eveneens', 0.626492977142334),\n",
       " ('tevens', 0.6048368811607361),\n",
       " ('cralingen', 0.6007931232452393),\n",
       " ('notabelenvergadering', 0.5996974110603333),\n",
       " ('evenals', 0.597119927406311),\n",
       " ('burense', 0.59560626745224),\n",
       " ('voocht', 0.5947650671005249)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"van\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
