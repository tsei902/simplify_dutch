{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39mstr\u001b[39m(Path(\u001b[39m__file__\u001b[39;49m)\u001b[39m.\u001b[39mresolve()\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mparent))\n\u001b[0;32m      5\u001b[0m \u001b[39m# -- end fix path --\u001b[39;00m\n\u001b[0;32m      6\u001b[0m RESOURCES_DIR \u001b[39m=\u001b[39m Path(\u001b[39m'\u001b[39m\u001b[39m./resources\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# -- fix path --\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path(__file__).resolve().parent.parent))\n",
    "# -- end fix path --\n",
    "\n",
    "# import sys \n",
    "# import os\n",
    "# sys.path.append(os.path.abspath(\"/Users/hp/Documents/AFR\"))\n",
    "# import test_standard.py\n",
    "\n",
    "RESOURCES_DIR = Path('./resources')\n",
    "DATASETS_PATH = RESOURCES_DIR / \"datasets\"\n",
    "WORD_EMBEDDINGS_NAME ='conllmodel'\n",
    "DUMPS_DIR = RESOURCES_DIR / \"DUMPS\"\n",
    "\n",
    "\n",
    "# from paths import DUMPS_DIR, ASSET_DATASET,  PHASES, get_data_filepath, PROCESSED_DATA_DIR, \\\n",
    "#     DATASETS_DIR, WIKILARGE_DATASET, WORD_EMBEDDINGS_NAME \n",
    "#     # WORD_FREQUENCY_FILEPATH \n",
    "# from source.utils import tokenize, yield_lines, load_dump, dump, write_lines, count_line, \\\n",
    "#     print_execution_time, save_preprocessor, yield_sentence_pair\n",
    "    \n",
    "from functools import lru_cache\n",
    "from multiprocessing import Pool\n",
    "from string import punctuation\n",
    "import multiprocessing\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "import nltk\n",
    "import tarfile\n",
    "import zipfile\n",
    "import urllib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import gensim\n",
    "import time\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "# import source.preprocessor\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words('dutch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_execution_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 266\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[39m@lru_cache\u001b[39m(maxsize\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m6\u001b[39m)\n\u001b[0;32m    262\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mspacy_process\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m    263\u001b[0m         \u001b[39mreturn\u001b[39;00m get_spacy_model()(text)\n\u001b[1;32m--> 266\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPreprocessor\u001b[39;00m:\n\u001b[0;32m    267\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, features_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    268\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "Cell \u001b[1;32mIn[32], line 337\u001b[0m, in \u001b[0;36mPreprocessor\u001b[1;34m()\u001b[0m\n\u001b[0;32m    334\u001b[0m     queue\u001b[39m.\u001b[39mput(\u001b[39m1\u001b[39m)\n\u001b[0;32m    335\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_sentence_pair(complex_sent, simple_sent)\n\u001b[1;32m--> 337\u001b[0m \u001b[39m@print_execution_time\u001b[39m\n\u001b[0;32m    338\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_file_pair\u001b[39m(\u001b[39mself\u001b[39m, complex_filepath, simple_filepath):\n\u001b[0;32m    339\u001b[0m     \u001b[39m# print(f\"Preprocessing file: {complex_filepath}\")\u001b[39;00m\n\u001b[0;32m    340\u001b[0m     processed_complex_sentences \u001b[39m=\u001b[39m []\n\u001b[0;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_count \u001b[39m=\u001b[39m count_line(simple_filepath)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'print_execution_time' is not defined"
     ]
    }
   ],
   "source": [
    "def round(val):\n",
    "    return '%.2f' % val\n",
    "\n",
    "def safe_division(a, b):\n",
    "    return a / b if b else 0\n",
    "\n",
    "# def tokenize(sentence):\n",
    "#     return sentence.split()\n",
    "\n",
    "@lru_cache(maxsize=1024)\n",
    "def is_punctuation(word):\n",
    "    return ''.join([char for char in word if char not in punctuation]) == ''\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def remove_punctuation(text):\n",
    "    return ' '.join([word for word in tokenize(text) if not is_punctuation(word)])\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([w for w in tokenize(text) if w.lower() not in stopwords])\n",
    "\n",
    "@lru_cache(maxsize=1024)\n",
    "def get_dependency_tree_depth(sentence):\n",
    "    def tree_height(node):\n",
    "        if len(list(node.children)) == 0:\n",
    "            return 0\n",
    "        return 1 + max(tree_height(child) for child in node.children)\n",
    "\n",
    "    tree_depths = [tree_height(spacy_sentence.root) for spacy_sentence in spacy_process(sentence).sents]\n",
    "    if len(tree_depths) == 0:\n",
    "        return 0\n",
    "    return max(tree_depths)\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_spacy_model():\n",
    "    model = 'nl_core_news_sm'  # from spacy, Dutch pipeline optimized for CPU. Components: tok2vec, morphologizer, tagger, parser, lemmatizer (trainable_lemmatizer), senter, ner.\n",
    "    if not spacy.util.is_package(model):\n",
    "        spacy.cli.download(model)\n",
    "        spacy.cli.link(model, model, force=True, model_path=spacy.util.get_package_path(model))\n",
    "    return spacy.load(model)\n",
    "\n",
    "@lru_cache(maxsize=10 ** 6)\n",
    "def spacy_process(text):\n",
    "    return get_spacy_model()(str(text))\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_word2rank(vocab_size=np.inf):\n",
    "    model_filepath = DUMPS_DIR / f\"{WORD_EMBEDDINGS_NAME}.pk\"\n",
    "    if model_filepath.exists():\n",
    "        return load_dump(model_filepath)\n",
    "    else:\n",
    "        print(\"Downloading dutch embeddings ...\") # pretrained vectors\n",
    "        download_twitter_embeddings(model_name='coostco', dest_dir=str(DUMPS_DIR))\n",
    "        print(\"Preprocessing word2rank...\")\n",
    "        DUMPS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        WORD_EMBEDDINGS_PATH = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.bin'\n",
    "        model = load_word_embeddings(WORD_EMBEDDINGS_PATH) # returns index_to_key\n",
    "        # store into file\n",
    "        lines_generator = model \n",
    "        word2rank = {}\n",
    "        print('vocab_size', vocab_size)\n",
    "        for i, line in enumerate(lines_generator):\n",
    "            if i >= vocab_size: break # its not vocab size any more but  # len(model.key_to_index)\n",
    "            word = line.split(',')[0]\n",
    "            word2rank[word] = i\n",
    "        pickle.dump(word2rank, open(model_filepath, 'wb'))\n",
    "        # txt_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "        # zip_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.zip'\n",
    "    return word2rank        \n",
    "    \n",
    "def load_word_embeddings(filepath):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(filepath, binary=True) # '../resources/DUMPS/model.bin'\n",
    "    model_indexes = model.index_to_key\n",
    "    return model_indexes   \n",
    "    \n",
    "def download_twitter_embeddings(model_name, dest_dir): # pretrained rankings\n",
    "    url = ''\n",
    "    if model_name == 'coosto_model':\n",
    "        url = 'https://github.com/coosto/dutch-word-embeddings/releases/download/v1.0/model.bin'\n",
    "    file_path = download_url(url, dest_dir)\n",
    "    out_filepath = Path(file_path)\n",
    "    out_filepath = out_filepath.parent / f'{out_filepath.stem}.txt'\n",
    "    # print(out_filepath, out_filepath.exists())\n",
    "    if not out_filepath.exists():\n",
    "        print(\"Extracting: \", Path(file_path).name)\n",
    "        unzip(file_path, dest_dir) \n",
    "        \n",
    "def download_url(self, url, output_path):\n",
    "        name = url.split('/')[-1]\n",
    "        file_path = f'{output_path}/{name}'\n",
    "        if not Path(file_path).exists():\n",
    "            with tqdm(unit='B', unit_scale=True, leave=True, miniters=1,desc=name) as t:  # all optional kwargs\n",
    "                urllib.request.urlretrieve(url, filename=file_path, reporthook=self._download_report_hook(t), data=None)\n",
    "        return file_path   \n",
    "    \n",
    "def unzip(self, file_path, dest_dir=None):\n",
    "    if dest_dir is None:\n",
    "        dest_dir = os.path.dirname(file_path)\n",
    "    if file_path.endswith('.zip'):\n",
    "        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(dest_dir)\n",
    "    elif file_path.endswith(\"tar.gz\") or file_path.endswith(\"tgz\"):\n",
    "        tar = tarfile.open(file_path, \"r:gz\")\n",
    "        tar.extractall(dest_dir)\n",
    "        tar.close()\n",
    "    elif file_path.endswith(\"tar\"):\n",
    "        tar = tarfile.open(file_path, \"r:\")\n",
    "        tar.extractall(dest_dir)\n",
    "        tar.close()\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def get_normalized_rank(word):\n",
    "    max = len(get_word2rank())\n",
    "    rank = get_word2rank().get(word, max)\n",
    "    return np.log(1 + rank) / np.log(1 + max)\n",
    "    # return np.log(1 + rank)\n",
    "    \n",
    "@lru_cache(maxsize=2048)\n",
    "def get_complexity_score2(sentence):\n",
    "    words = tokenize(remove_stopwords(remove_punctuation(sentence)))\n",
    "    words = [word for word in words if word in get_word2rank()]  # remove unknown words\n",
    "    if len(words) == 0:\n",
    "        return 1.0\n",
    "    return np.array([get_normalized_rank(word) for word in words]).mean()\n",
    "\n",
    "# @lru_cache(maxsize=1)\n",
    "# def get_word_frequency():\n",
    "#     model_filepath = DUMPS_DIR / f'{WORD_FREQUENCY_FILEPATH.stem}.pk'\n",
    "#     if model_filepath.exists():\n",
    "#         return load_dump(model_filepath)\n",
    "#     else:\n",
    "#         DUMPS_DIR.mkdir(parents=True, exist_ok=True) \n",
    "#         word_freq = {}\n",
    "#         for line in yield_lines(WORD_FREQUENCY_FILEPATH):\n",
    "#             chunks = line.split(' ')\n",
    "#             word = chunks[0]\n",
    "#             freq = int(chunks[1])\n",
    "#             word_freq[word] = freq\n",
    "#         dump(word_freq, model_filepath)\n",
    "#         return word_freq\n",
    "\n",
    "# @lru_cache(maxsize=10000)\n",
    "# def get_normalized_frequency(word):\n",
    "#     max = 153141437 # the 153141437, the max frequency\n",
    "#     freq = get_word_frequency().get(word, 0)\n",
    "#     return 1.0 - np.log(1 + freq) / np.log(1 + max)\n",
    "\n",
    "\n",
    "# @lru_cache(maxsize=2048)\n",
    "# def get_complexity_score(sentence):\n",
    "#     # words = tokenize(remove_stopwords(remove_punctuation(sentence)))\n",
    "#     words = tokenize(remove_punctuation(sentence))\n",
    "#     words = [word for word in words if word in get_word2rank()]  # remove unknown words\n",
    "#     if len(words) == 0:\n",
    "#         return 1.0\n",
    "    \n",
    "#     return np.array([get_normalized_frequency(word.lower()) for word in words]).mean()\n",
    "\n",
    "def download_requirements():\n",
    "    get_spacy_model()\n",
    "    get_word2rank()\n",
    "\n",
    "class RatioFeature:\n",
    "    def __init__(self, feature_extractor, target_ratio=0.8):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.target_ratio = target_ratio\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        return f'{self.name}_{self.target_ratio}'\n",
    "\n",
    "    def encode_sentence_pair(self, complex_sentence, simple_sentence):\n",
    "        return f'{self.name}_{self.feature_extractor(complex_sentence, simple_sentence)}', simple_sentence\n",
    "\n",
    "    def decode_sentence(self, encoded_sentence):\n",
    "        return encoded_sentence\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        class_name = self.__class__.__name__.replace('RatioFeature', '')\n",
    "        name = \"\"\n",
    "        for word in re.findall('[A-Z][^A-Z]*', class_name):\n",
    "            if word: name += word[0]\n",
    "        if not name: name = class_name\n",
    "        return name\n",
    "\n",
    "class WordRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_word_length_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_word_length_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(safe_division(len(tokenize(simple_sentence)), len(tokenize(complex_sentence))))\n",
    "\n",
    "\n",
    "class CharRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_char_length_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_char_length_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(safe_division(len(simple_sentence), len(complex_sentence)))\n",
    "\n",
    "\n",
    "class LevenshteinRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_levenshtein_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_levenshtein_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(Levenshtein.ratio(complex_sentence, simple_sentence))\n",
    "\n",
    "class WordRankRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_word_rank_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_word_rank_ratio(self, complex_sentence, simple_sentence):\n",
    "        score = round(min(safe_division(self.get_lexical_complexity_score(simple_sentence),\n",
    "                                       self.get_lexical_complexity_score(complex_sentence)), 2))\n",
    "        print('score', score)\n",
    "        return score\n",
    "\n",
    "    def get_lexical_complexity_score(self, sentence):\n",
    "        print('enter lexical loop')\n",
    "        words = tokenize(remove_stopwords(remove_punctuation(sentence)))\n",
    "        print('sentence \"tokenization\" into individal words', words)\n",
    "        words = [word for word in words if word in get_word2rank()]\n",
    "        print('words here is the check if the word exists?', words)\n",
    "        print('still in lexical loop')\n",
    "        if len(words) == 0:\n",
    "            return np.log(1 + len(get_word2rank()))\n",
    "        score =  np.quantile([self.get_rank(word) for word in words], 0.75)\n",
    "        print('score for each word', score)\n",
    "        print('lexical compexity score', score)\n",
    "        return score\n",
    "\n",
    "    @lru_cache(maxsize=5000)\n",
    "    def get_rank(self, word):\n",
    "        rank = get_word2rank().get(word, len(get_word2rank()))\n",
    "        print('rank of word from word2rank - glove ', rank)\n",
    "        ranker = np.log(1 + rank)\n",
    "        print('ranker: ', ranker)\n",
    "        return ranker\n",
    "\n",
    "class DependencyTreeDepthRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_dependency_tree_depth_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_dependency_tree_depth_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(\n",
    "            safe_division(self.get_dependency_tree_depth(simple_sentence),\n",
    "                          self.get_dependency_tree_depth(complex_sentence)))\n",
    "\n",
    "    @lru_cache(maxsize=1024)\n",
    "    def get_dependency_tree_depth(self, sentence):\n",
    "        def get_subtree_depth(node):\n",
    "            if len(list(node.children)) == 0:\n",
    "                return 0\n",
    "            return 1 + max([get_subtree_depth(child) for child in node.children])\n",
    "\n",
    "        tree_depths = [get_subtree_depth(spacy_sentence.root) for spacy_sentence in self.spacy_process(sentence).sents]\n",
    "        if len(tree_depths) == 0:\n",
    "            return 0\n",
    "        return max(tree_depths)\n",
    "\n",
    "    @lru_cache(maxsize=10 ** 6)\n",
    "    def spacy_process(self, text):\n",
    "        return get_spacy_model()(text)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, features_kwargs=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = self.get_features(features_kwargs)\n",
    "        if features_kwargs:\n",
    "            self.hash = utils.generate_hash(str(features_kwargs).encode())\n",
    "        else:\n",
    "            self.hash = \"no_feature\"\n",
    "\n",
    "    def get_class(self, class_name, *args, **kwargs):\n",
    "        return globals()[class_name](*args, **kwargs)\n",
    "\n",
    "    def get_features(self, feature_kwargs):\n",
    "        features = []\n",
    "        for feature_name, kwargs in feature_kwargs.items():\n",
    "            features.append(self.get_class(feature_name, **kwargs))\n",
    "        return features\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        if self.features:\n",
    "            line = ''\n",
    "            for feature in self.features:\n",
    "                line += feature.encode_sentence(sentence) + ' '\n",
    "            line += ' ' + sentence\n",
    "            return line.rstrip()\n",
    "        else:\n",
    "            return sentence\n",
    "\n",
    "    def encode_sentence_pair(self, complex_sentence, simple_sentence):\n",
    "        # print(complex_sentence)\n",
    "        if self.features:\n",
    "            line = ''\n",
    "            for feature in self.features:\n",
    "                # startTime = timeit.default_timer()\n",
    "                # print(feature)\n",
    "                processed_complex, _ = feature.encode_sentence_pair(complex_sentence, simple_sentence)\n",
    "                line += processed_complex + ' '\n",
    "                print('featured sentence', line)\n",
    "                # print(feature, timeit.default_timer() - startTime)\n",
    "            line += ' ' + complex_sentence\n",
    "            return line.rstrip()\n",
    "\n",
    "        else:\n",
    "            return complex_sentence\n",
    "\n",
    "    def decode_sentence(self, encoded_sentence):\n",
    "        for feature in self.features:\n",
    "            decoded_sentence = feature.decode_sentence(encoded_sentence)\n",
    "        return decoded_sentence\n",
    "\n",
    "    def encode_file(self, input_filepath, output_filepath):\n",
    "        with open(output_filepath, 'w') as f:\n",
    "            for line in yield_lines(input_filepath):\n",
    "                f.write(self.encode_sentence(line) + '\\n')\n",
    "\n",
    "    def decode_file(self, input_filepath, output_filepath):\n",
    "        with open(output_filepath, 'w') as f:\n",
    "            for line in yield_lines(input_filepath):\n",
    "                f.write(self.decode_sentence(line) + '\\n')\n",
    "\n",
    "    def process_encode_sentence_pair(self, sentences):\n",
    "        print(f\"{sentences[2]}/{self.line_count}\", sentences[0])  # sentence[0] index\n",
    "        return (self.encode_sentence_pair(sentences[0], sentences[1]))\n",
    "\n",
    "    def pool_encode_sentence_pair(self, args):\n",
    "        # print(f\"{processed_line_count}/{self.line_count}\")\n",
    "        complex_sent, simple_sent, queue = args\n",
    "        queue.put(1)\n",
    "        return self.encode_sentence_pair(complex_sent, simple_sent)\n",
    "\n",
    "    @print_execution_time\n",
    "    def encode_file_pair(self, complex_filepath, simple_filepath):\n",
    "        # print(f\"Preprocessing file: {complex_filepath}\")\n",
    "        processed_complex_sentences = []\n",
    "        self.line_count = count_line(simple_filepath)\n",
    "\n",
    "        nb_cores = multiprocessing.cpu_count()\n",
    "        manager = multiprocessing.Manager()\n",
    "        queue = manager.Queue()\n",
    "\n",
    "        pool = Pool(processes=nb_cores)\n",
    "        args = [(complex_sent, simple_sent, queue) for complex_sent, simple_sent in\n",
    "                yield_sentence_pair(complex_filepath, simple_filepath)]\n",
    "        res = pool.map_async(self.pool_encode_sentence_pair, args)\n",
    "        while not res.ready():\n",
    "            # remaining = res._number_left * res._chunksize\n",
    "            size = queue.qsize()\n",
    "            print(f\"Preprocessing: {size} / {self.line_count}\")\n",
    "            time.sleep(0.5)\n",
    "        encoded_sentences = res.get()\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        # pool.terminate()\n",
    "        # i = 0\n",
    "        # for complex_sentence, simple_sentence in yield_sentence_pair(complex_filepath, simple_filepath):\n",
    "        # # print(complex_sentence)\n",
    "        #     processed_complex_sentence = self.encode_sentence_pair(complex_sentence, simple_sentence)\n",
    "        #     i +=1\n",
    "        #     print(f\"{i}/{self.line_count}\", processed_complex_sentence)\n",
    "        # processed_complex_sentences.append(encoded_complex)\n",
    "\n",
    "        return encoded_sentences\n",
    "\n",
    "    def get_preprocessed_filepath(self, dataset, phase, type):\n",
    "        filename = f'{dataset}.{phase}.{type}'\n",
    "        return self.preprocessed_data_dir / filename\n",
    "\n",
    "    def preprocess_dataset(self, dataset):\n",
    "        # download_requirements()\n",
    "        # print('self.hash', self.hash)\n",
    "        print('dataset', dataset)\n",
    "        self.preprocessed_data_dir = PROCESSED_DATA_DIR /  dataset #self.hash /\n",
    "        self.preprocessed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        save_preprocessor(self)\n",
    "        print(f'Preprocessing dataset: {dataset}')\n",
    "\n",
    "        for phase in PHASES:\n",
    "            # for phase in [\"train\", \"valid\"]: \n",
    "            complex_filepath = get_data_filepath(dataset, phase, 'orig')\n",
    "            simple_filepath = get_data_filepath(dataset, phase, 'simp')\n",
    "\n",
    "            complex_output_filepath = self.preprocessed_data_dir / complex_filepath.name\n",
    "            simple_output_filepath = self.preprocessed_data_dir / simple_filepath.name\n",
    "            if complex_output_filepath.exists() and simple_output_filepath.exists():\n",
    "                continue\n",
    "\n",
    "            print(f'Prepocessing files: {complex_filepath.name} {simple_filepath.name}')\n",
    "            processed_complex_sentences = self.encode_file_pair(complex_filepath, simple_filepath)\n",
    "\n",
    "            write_lines(processed_complex_sentences, complex_output_filepath)\n",
    "            shutil.copy(simple_filepath, simple_output_filepath)\n",
    "\n",
    "        print(f'Preprocessing dataset \"{dataset}\" is finished.')\n",
    "        return self.preprocessed_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    features_kwargs = {\n",
    "        'WordRatioFeature': {'target_ratio': 0.8},\n",
    "        'CharRatioFeature': {'target_ratio': 0.8},\n",
    "        'LevenshteinRatioFeature': {'target_ratio': 0.8},\n",
    "        'WordRankRatioFeature': {'target_ratio': 0.8},\n",
    "        'DependencyTreeDepthRatioFeature': {'target_ratio': 0.8}\n",
    "    }\n",
    "    # features_kwargs = {}\n",
    "    preprocessor = Preprocessor(features_kwargs)\n",
    "    # preprocessor.preprocess_dataset(ASSET_DATASET)\n",
    "    preprocessor.preprocess_dataset(WIKILARGE_DATASET)\n",
    "    # preprocessor.preprocess_dataset(NEWSELA_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_pair = {\"simple_text\": \"Dit is de complexe versie en meer contextuele informatie.\", \"original_text\": \"Deze is de makkelijke versie.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordRankRatioFeature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word_rank \u001b[39m=\u001b[39m WordRankRatioFeature()\n\u001b[0;32m      2\u001b[0m sentences_pair \u001b[39m=\u001b[39m word_rank\u001b[39m.\u001b[39mget_ratio(sentences_pair)\n\u001b[0;32m      3\u001b[0m sentences_pair\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WordRankRatioFeature' is not defined"
     ]
    }
   ],
   "source": [
    "word_rank = WordRankRatioFeature()\n",
    "sentences_pair = word_rank.get_ratio(sentences_pair)\n",
    "sentences_pair"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
