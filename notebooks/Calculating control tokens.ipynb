{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paths'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msource\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpaths\u001b[39;00m \u001b[39mimport\u001b[39;00m DUMPS_DIR, PHASES, get_data_filepath, PROCESSED_DATA_DIR,WIKILARGE_DATASET, WORD_EMBEDDINGS_NAME \n\u001b[0;32m     23\u001b[0m     \u001b[39m# WORD_FREQUENCY_FILEPATH\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msource\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m tokenize, yield_lines, load_dump, write_lines, count_line, \\\n\u001b[0;32m     25\u001b[0m   print_execution_time, save_preprocessor, yield_sentence_pair\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m Pool\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstring\u001b[39;00m \u001b[39mimport\u001b[39;00m punctuation\n",
      "File \u001b[1;32mc:\\Users\\Theresa\\OneDrive - KU Leuven\\Documents\\GitHub\\simplify_dutch\\source\\utils.py:18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[0;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhashlib\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpaths\u001b[39;00m \u001b[39mimport\u001b[39;00m DUMPS_DIR, WIKILARGE_DATASET, get_temp_filepath, get_data_filepath\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msacremoses\u001b[39;00m \u001b[39mimport\u001b[39;00m MosesDetokenizer, MosesTokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'paths'"
     ]
    }
   ],
   "source": [
    "# # -- fix path --\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# sys.path.append(str(Path(__file__).resolve().parent.parent))\n",
    "# -- end fix path --\n",
    "\n",
    "\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/Users/Theresa/OneDrive - KU Leuven/Documents/GitHub/simplify_dutch/\"))\n",
    "import source\n",
    "# import sys\n",
    "# for p in sys.path:\n",
    "#     print(p)\n",
    "# import test_standard.py\n",
    "\n",
    "RESOURCES_DIR = Path('../resources')\n",
    "DATASETS_PATH = RESOURCES_DIR / \"datasets\"\n",
    "WORD_EMBEDDINGS_NAME ='conllmodel'\n",
    "DUMPS_DIR = RESOURCES_DIR / \"DUMPS\"\n",
    "\n",
    "from source.paths import DUMPS_DIR, PHASES, get_data_filepath, PROCESSED_DATA_DIR,WIKILARGE_DATASET, WORD_EMBEDDINGS_NAME \n",
    "    # WORD_FREQUENCY_FILEPATH\n",
    "from source.utils import tokenize, yield_lines, load_dump, write_lines, count_line, \\\n",
    "  print_execution_time, save_preprocessor, yield_sentence_pair\n",
    "    \n",
    "from multiprocessing import Pool\n",
    "from string import punctuation\n",
    "import multiprocessing\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "import nltk\n",
    "import tarfile\n",
    "import zipfile\n",
    "import urllib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import gensim\n",
    "import time\n",
    "# from nltk import word_tokenize\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "# import source.preprocessor\n",
    "from compound_split import doc_split\n",
    "\n",
    "stopwords = set(stopwords.words('dutch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://vectors.nlpl.eu/repository/# # conll dataset\n",
    "# Current model is CONLL17 model - best results so far.\n",
    "# does not provide a download link thus needs to be done manually\n",
    "\n",
    "# https://github.com/coosto/dutch-word-embeddings\n",
    "# https://fasttext.cc/docs/en/crawl-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ControlDivisionByZero(numerator, denominator):\n",
    "#     return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "def round(val):\n",
    "    return '%.2f' % val\n",
    "\n",
    "def safe_division(a, b):\n",
    "    return a / b if b else 0\n",
    "\n",
    "# def tokenize(sentence):\n",
    "#     return sentence.split()\n",
    "\n",
    "\n",
    "def is_punctuation(word):\n",
    "    return ''.join([char for char in word if char not in punctuation]) == ''\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return ' '.join([word for word in source.utils.tokenize(text) if not is_punctuation(word)])\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([w for w in source.utils.tokenize(text) if w.lower() not in stopwords])\n",
    "\n",
    "\n",
    "# def get_dependency_tree_depth(sentence):\n",
    "#     def tree_height(node):\n",
    "#         if len(list(node.children)) == 0:\n",
    "#             return 0\n",
    "#         return 1 + max(tree_height(child) for child in node.children)\n",
    "\n",
    "#     tree_depths = [tree_height(spacy_sentence.root) for spacy_sentence in spacy_process(sentence).sents]\n",
    "#     if len(tree_depths) == 0:\n",
    "#         return 0\n",
    "#     return max(tree_depths)\n",
    "\n",
    "\n",
    "def get_spacy_model():\n",
    "    model = 'nl_core_news_sm'  # from spacy, Dutch pipeline optimized for CPU. Components: tok2vec, morphologizer, tagger, parser, lemmatizer (trainable_lemmatizer), senter, ner.\n",
    "    if not spacy.util.is_package(model):\n",
    "        spacy.cli.download(model)\n",
    "        spacy.cli.link(model, model, force=True, model_path=spacy.util.get_package_path(model))\n",
    "    return spacy.load(model)\n",
    "\n",
    "\n",
    "def spacy_process(text):\n",
    "    return get_spacy_model()(str(text))\n",
    "\n",
    "\n",
    "def get_word2rank(vocab_size=np.inf):\n",
    "    model_filepath = DUMPS_DIR / f\"{WORD_EMBEDDINGS_NAME}.pk\"\n",
    "    if model_filepath.exists():\n",
    "        return source.utils.load_dump(model_filepath)\n",
    "    else:\n",
    "        None\n",
    "        # # print(\"Downloading dutch embeddings ...\") # pretrained vectors\n",
    "        # download_twitter_embeddings(model_name='coostco', dest_dir=str(DUMPS_DIR))\n",
    "        # # print(\"Preprocessing word2rank...\")\n",
    "        # DUMPS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        # WORD_EMBEDDINGS_PATH = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.bin'\n",
    "        # model = load_word_embeddings(WORD_EMBEDDINGS_PATH) # returns index_to_key\n",
    "        # # store into file\n",
    "        # lines_generator = model \n",
    "        # word2rank = {}\n",
    "        # # print('vocab_size', vocab_size)\n",
    "        # for i, line in enumerate(lines_generator):\n",
    "        #     if i >= vocab_size: break # its not vocab size any more but  # len(model.key_to_index)\n",
    "        #     word = line.split(',')[0]\n",
    "        #     word2rank[word] = i\n",
    "        # pickle.dump(word2rank, open(model_filepath, 'wb'))\n",
    "        # # txt_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "        # # zip_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.zip'\n",
    "    # return word2rank        \n",
    "    \n",
    "def load_word_embeddings(filepath):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(filepath, binary=True) # '../resources/DUMPS/model.bin'\n",
    "    model_indexes = model.index_to_key\n",
    "    return model_indexes   \n",
    "    \n",
    "def download_twitter_embeddings(model_name, dest_dir): # pretrained rankings\n",
    "    url = ''\n",
    "    if model_name == 'coosto_model':\n",
    "        url = 'https://github.com/coosto/dutch-word-embeddings/releases/download/v1.0/model.bin'\n",
    "    file_path = download_url(url, dest_dir)\n",
    "    out_filepath = Path(file_path)\n",
    "    out_filepath = out_filepath.parent / f'{out_filepath.stem}.txt'\n",
    "    # print(out_filepath, out_filepath.exists())\n",
    "    if not out_filepath.exists():\n",
    "        # print(\"Extracting: \", Path(file_path).name)\n",
    "        unzip(file_path, dest_dir) \n",
    "        \n",
    "def download_url(self, url, output_path):\n",
    "        name = url.split('/')[-1]\n",
    "        file_path = f'{output_path}/{name}'\n",
    "        if not Path(file_path).exists():\n",
    "            with tqdm(unit='B', unit_scale=True, leave=True, miniters=1,desc=name) as t:  # all optional kwargs\n",
    "                urllib.request.urlretrieve(url, filename=file_path, reporthook=self._download_report_hook(t), data=None)\n",
    "        return file_path   \n",
    "    \n",
    "def unzip(self, file_path, dest_dir=None):\n",
    "    if dest_dir is None:\n",
    "        dest_dir = os.path.dirname(file_path)\n",
    "    if file_path.endswith('.zip'):\n",
    "        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(dest_dir)\n",
    "    elif file_path.endswith(\"tar.gz\") or file_path.endswith(\"tgz\"):\n",
    "        tar = tarfile.open(file_path, \"r:gz\")\n",
    "        tar.extractall(dest_dir)\n",
    "        tar.close()\n",
    "    elif file_path.endswith(\"tar\"):\n",
    "        tar = tarfile.open(file_path, \"r:\")\n",
    "        tar.extractall(dest_dir)\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "def get_normalized_rank(word):\n",
    "    max = len(get_word2rank())\n",
    "    rank = get_word2rank().get(word, max)\n",
    "    return np.log(1 + rank) / np.log(1 + max)\n",
    "    # return np.log(1 + rank)\n",
    "    \n",
    "\n",
    "# def get_complexity_score2(sentence):\n",
    "#     words = source.utils.tokenize(remove_stopwords(remove_punctuation(sentence)))\n",
    "#     words = [word for word in words if word in get_word2rank()]  # remove unknown words\n",
    "#     if len(words) == 0:\n",
    "#         return 1.0\n",
    "#     return np.array([get_normalized_rank(word) for word in words]).mean()\n",
    "\n",
    "# def get_word_frequency():\n",
    "#     model_filepath = DUMPS_DIR / f'{WORD_FREQUENCY_FILEPATH.stem}.pk'\n",
    "#     if model_filepath.exists():\n",
    "#         return load_dump(model_filepath)\n",
    "#     else:\n",
    "#         DUMPS_DIR.mkdir(parents=True, exist_ok=True) \n",
    "#         word_freq = {}\n",
    "#         for line in yield_lines(WORD_FREQUENCY_FILEPATH):\n",
    "#             chunks = line.split(' ')\n",
    "#             word = chunks[0]\n",
    "#             freq = int(chunks[1])\n",
    "#             word_freq[word] = freq\n",
    "#         dump(word_freq, model_filepath)\n",
    "#         return word_freq\n",
    "\n",
    "# def get_normalized_frequency(word):\n",
    "#     max = 153141437 # the 153141437, the max frequency\n",
    "#     freq = get_word_frequency().get(word, 0)\n",
    "#     return 1.0 - np.log(1 + freq) / np.log(1 + max)\n",
    "\n",
    "# def get_complexity_score(sentence):\n",
    "#     # words = tokenize(remove_stopwords(remove_punctuation(sentence)))\n",
    "#     words = tokenize(remove_punctuation(sentence))\n",
    "#     words = [word for word in words if word in get_word2rank()]  # remove unknown words\n",
    "#     if len(words) == 0:\n",
    "#         return 1.0\n",
    "    \n",
    "#     return np.array([get_normalized_frequency(word.lower()) for word in words]).mean()\n",
    "\n",
    "def download_requirements():\n",
    "    get_spacy_model()\n",
    "    get_word2rank()\n",
    "\n",
    "class RatioFeature:\n",
    "    def __init__(self, feature_extractor, target_ratio=0.8):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.target_ratio = target_ratio\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        return f'{self.name}_{self.target_ratio}'\n",
    "\n",
    "    def encode_sentence_pair(self, complex_sentence, simple_sentence):\n",
    "        return f'{self.name}_{self.feature_extractor(complex_sentence, simple_sentence)}' # , simple_sentence\n",
    "\n",
    "    def encode_sentence_pair_stripped(self, complex_sentence, simple_sentence):\n",
    "            return  float(self.feature_extractor(complex_sentence, simple_sentence)) # , simple_sentence\n",
    "\n",
    "    def decode_sentence(self, encoded_sentence):\n",
    "        return encoded_sentence\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        class_name = self.__class__.__name__.replace('RatioFeature', '')\n",
    "        name = \"\"\n",
    "        for word in re.findall('[A-Z][^A-Z]*', class_name):\n",
    "            if word: name += word[0]\n",
    "        if not name: name = class_name\n",
    "        return name\n",
    "\n",
    "class WordLengthRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_word_length_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_word_length_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(safe_division(len(source.utils.tokenize(simple_sentence)), len(source.utils.tokenize(complex_sentence))))\n",
    "\n",
    "\n",
    "class CharLengthRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_char_length_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_char_length_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(safe_division(len(simple_sentence), len(complex_sentence)))\n",
    "\n",
    "\n",
    "class LevenshteinRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_levenshtein_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_levenshtein_ratio(self, complex_sentence, simple_sentence):\n",
    "        print(type(complex_sentence))\n",
    "        \n",
    "        complex_sentence = tokenize(complex_sentence) # ,language='dutch')\n",
    "        # print(complex_sentence)\n",
    "        simple_sentence = tokenize(simple_sentence) # ,language='dutch')\n",
    "        # print(simple_sentence)\n",
    "        return round(Levenshtein.seqratio(complex_sentence, simple_sentence))\n",
    "\n",
    "class WordRankRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_word_rank_ratio, *args, **kwargs)\n",
    "\n",
    "    def get_word_rank_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(min(safe_division(self.get_lexical_complexity_score(simple_sentence),\n",
    "                                       self.get_lexical_complexity_score(complex_sentence)), 2))\n",
    "\n",
    "    def get_lexical_complexity_score(self, sentence):\n",
    "        # print('enter lexical loop')\n",
    "        words = source.utils.tokenize(remove_stopwords(remove_punctuation(sentence)))\n",
    "        # print(type(words))\n",
    "        # print('sentence \"tokenization\" into individal words', words)\n",
    "        words2 = doc_split.maximal_split(words)\n",
    "        # print(type(words2))\n",
    "        # print('result nach max split', words2)\n",
    "        words = [word for word in words if word in get_word2rank()]\n",
    "        # print('words here is the check if the word exists ', words)\n",
    "        # print('still in lexical loop')\n",
    "        if len(words) == 0:\n",
    "            return np.log(1 + len(get_word2rank()))\n",
    "         #intermediate = [self.get_rank(word) for word in words]\n",
    "        # print('log rank scores of all words', intermediate)\n",
    "        return np.quantile([self.get_rank(word) for word in words], 0.75)\n",
    "        # print('log rank score for each word and third quartile of it', score)\n",
    "        # print('lexical compexity score', score)\n",
    "        # return score\n",
    "\n",
    "    def get_rank(self, word):\n",
    "        rank = get_word2rank().get(word, len(get_word2rank()))\n",
    "        print('rank of word from word2rank - glove ', rank)\n",
    "        ranker = np.log(1 + rank)\n",
    "        print('ranker: ', ranker)\n",
    "        return ranker\n",
    "    \n",
    "class WordRankRatio2(RatioFeature):\n",
    "    # single underscore = internally\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_word_rank_ratio, *args, **kwargs)\n",
    "        # self.tokenizer = MosesTokenizer(lang='nl')\n",
    "\n",
    "    def get_word_rank_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(min(safe_division(self.get_lexical_complexity_score(simple_sentence),\n",
    "                                       self.get_lexical_complexity_score(complex_sentence)), 2))\n",
    "\n",
    "    def get_lexical_complexity_score(self, sentence):\n",
    "        # print('enter lexical loop')\n",
    "        words = source.utils.tokenize(self._remove_stopwords(self._remove_punctuation(sentence)))\n",
    "        # print(type(words))\n",
    "        # print('sentence \"tokenization\" into individal words', words)\n",
    "        # words2 = doc_split.maximal_split(words)\n",
    "        # print(type(words2))\n",
    "        # print('result nach max split', words2)\n",
    "        words = [word for word in words if word in get_word2rank()]\n",
    "        # print('words here is the check if the word exists ', words)\n",
    "        # print('still in lexical loop')\n",
    "        if len(words) == 0:\n",
    "            return np.log(1 + len(get_word2rank()))\n",
    "         #intermediate = [self.get_rank(word) for word in words]\n",
    "        # print('log rank scores of all words', intermediate)\n",
    "        return np.quantile([self.get_rank(word) for word in words], 0.75)\n",
    "        # print('log rank score for each word and third quartile of it', score)\n",
    "        # print('lexical compexity score', score)\n",
    "        # return score\n",
    "\n",
    "    def get_rank(self, word):\n",
    "        rank = get_word2rank().get(word, len(get_word2rank()))\n",
    "        # print('rank of word from word2rank - glove ', rank)\n",
    "        ranker = np.log(1 + rank)\n",
    "        #print('ranker: ', ranker)\n",
    "        return ranker\n",
    "\n",
    "    def _remove_punctuation(self, text):\n",
    "        return ' '.join([word for word in source.utils.tokenize(text) if not self._is_punctuation(word)])\n",
    "\n",
    "    def _remove_stopwords(self, text):\n",
    "        return ' '.join([w for w in source.utils.tokenize(text) if w.lower() not in stopwords])\n",
    "\n",
    "    def _is_punctuation(self, word):\n",
    "        return ''.join([char for char in word if char not in punctuation]) == ''\n",
    "\n",
    "    def _get_rank(self, word):\n",
    "        rank = self.word2rank.get(word, self.length_rank)\n",
    "        return np.log(1 + rank)\n",
    "\n",
    "    def _get_word2rank(self, vocab_size=np.inf):\n",
    "        model_filepath = DUMPS_DIR / f\"{WORD_EMBEDDINGS_NAME}.pk\"\n",
    "        if model_filepath.exists():\n",
    "            with open(model_filepath, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            return model\n",
    "        else:\n",
    "            print(\"Processing\", f\"{WORD_EMBEDDINGS_NAME}...\")\n",
    "            print(\"Preprocessing word2rank...\")\n",
    "            DUMPS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "            WORD_EMBEDDINGS_PATH = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "            lines_generator = self._yield_lines(WORD_EMBEDDINGS_PATH)\n",
    "            word2rank = {}\n",
    "            # next(lines_generator)\n",
    "            for i, line in enumerate(lines_generator):\n",
    "                if i >= vocab_size: break\n",
    "                word = line.split(' ')[0]\n",
    "                word2rank[word] = i\n",
    "\n",
    "            pickle.dump(word2rank, open(model_filepath, 'wb'))\n",
    "            # txt_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "            # zip_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.zip'\n",
    "            # if txt_file.exists(): txt_file.unlink()\n",
    "            # if zip_file.exists(): zip_file.unlink()\n",
    "            return word2rank\n",
    "        \n",
    "        # else:            \n",
    "        #     print(\"Downloading dutch embeddings ...\") # pretrained vectors\n",
    "        #     self._download_embeddings(model_name='coostco', dest_dir=str(DUMPS_DIR))\n",
    "        #     print(\"Preprocessing word2rank...\")\n",
    "        #     DUMPS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        #     WORD_EMBEDDINGS_PATH = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.bin'\n",
    "        #     model = self._load_word_embeddings(WORD_EMBEDDINGS_PATH) # returns index_to_key\n",
    "        #     # store into file\n",
    "        #     lines_generator = model # self._yield_lines(model) # (WORD_EMBEDDINGS_PATH)\n",
    "            \n",
    "        #     word2rank = {}\n",
    "        #     # next(lines_generator)\n",
    "        #     print('vocab_size', vocab_size)\n",
    "        #     for i, line in enumerate(lines_generator):\n",
    "        #         if i >= vocab_size: break # its not vocab size any more but  # len(model.key_to_index)\n",
    "        #         word = line.split(',')[0]\n",
    "        #         print('word', word)\n",
    "        #         word2rank[word] = i\n",
    "        #         print('ranked word?', word2rank[word])\n",
    "                \n",
    "        #     pickle.dump(word2rank, open(model_filepath, 'wb'))\n",
    "        #     txt_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "        #     zip_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.zip'\n",
    "        #     # if txt_file.exists(): txt_file.unlink()\n",
    "        #     # if zip_file.exists(): zip_file.unlink()\n",
    "        #     # print(word2rank)\n",
    "        #     return word2rank\n",
    "    \n",
    "    def _load_word_embeddings(self, filepath):\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(filepath, binary=True) # '../resources/DUMPS/model.bin'\n",
    "        model_indexes = model.index_to_key\n",
    "        return model_indexes\n",
    "\n",
    "    def _download_embeddings(self, model_name, dest_dir): # pretrained rankings\n",
    "        url = ''\n",
    "        if model_name == 'coost0_model':\n",
    "            url = 'https://github.com/coosto/dutch-word-embeddings/releases/download/v1.0/model.bin'\n",
    "\n",
    "        file_path = self._download_url(url, dest_dir)\n",
    "        out_filepath = Path(file_path)\n",
    "        out_filepath = out_filepath.parent / f'{out_filepath.stem}.txt'\n",
    "        # print(out_filepath, out_filepath.exists())\n",
    "        if not out_filepath.exists():\n",
    "            print(\"Extracting: \", Path(file_path).name)\n",
    "            self._unzip(file_path, dest_dir)\n",
    "\n",
    "    def _yield_lines(self, filepath):\n",
    "        filepath = Path(filepath)\n",
    "        with filepath.open('r', encoding=\"latin-1\") as f:\n",
    "            for line in f:\n",
    "                print(line)\n",
    "                yield line.rstrip()\n",
    "\n",
    "    def _download_url(self, url, output_path):\n",
    "        name = url.split('/')[-1]\n",
    "        file_path = f'{output_path}/{name}'\n",
    "        if not Path(file_path).exists():\n",
    "            with tqdm(unit='B', unit_scale=True, leave=True, miniters=1,\n",
    "                      desc=name) as t:  # all optional kwargs\n",
    "                urllib.request.urlretrieve(url, filename=file_path, reporthook=self._download_report_hook(t), data=None)\n",
    "        return file_path\n",
    "\n",
    "    def _unzip(self, file_path, dest_dir=None):\n",
    "        if dest_dir is None:\n",
    "            dest_dir = os.path.dirname(file_path)\n",
    "        if file_path.endswith('.zip'):\n",
    "            with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(dest_dir)\n",
    "        elif file_path.endswith(\"tar.gz\") or file_path.endswith(\"tgz\"):\n",
    "            tar = tarfile.open(file_path, \"r:gz\")\n",
    "            tar.extractall(dest_dir)\n",
    "            tar.close()\n",
    "        elif file_path.endswith(\"tar\"):\n",
    "            tar = tarfile.open(file_path, \"r:\")\n",
    "            tar.extractall(dest_dir)\n",
    "            tar.close()\n",
    "\n",
    "    def _download_report_hook(self, t):\n",
    "        last_b = [0]\n",
    "\n",
    "        def inner(b=1, bsize=1, tsize=None):\n",
    "            if tsize is not None:\n",
    "                t.total = tsize\n",
    "            t.update((b - last_b[0]) * bsize)\n",
    "            last_b[0] = b\n",
    "\n",
    "        return inner\n",
    "    \n",
    "    \n",
    "# adjusted from CLEF\n",
    "class DependencyTreeDepthRatioFeature(RatioFeature):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(self.get_dependency_tree_depth_ratio, *args, **kwargs)\n",
    "        self.nlp = self.get_spacy_model() # adjusted from CLEF\n",
    "\n",
    "    def get_spacy_model(self):\n",
    "        model = 'nl_core_news_sm'  # from spacy, Dutch pipeline optimized for CPU. Components: tok2vec, morphologizer, tagger, parser, lemmatizer (trainable_lemmatizer), senter, ner.\n",
    "        if not spacy.util.is_package(model):\n",
    "            spacy.cli.download(model)\n",
    "            spacy.cli.link(model, model, force=True, model_path=spacy.util.get_package_path(model))\n",
    "        return spacy.load(model)\n",
    "    \n",
    "    def get_dependency_tree_depth_ratio(self, complex_sentence, simple_sentence):\n",
    "        return round(safe_division(self.get_dependency_tree_depth(simple_sentence),\n",
    "                    self.get_dependency_tree_depth(complex_sentence)))\n",
    "        # score =  round(ControlDivisionByZero(self.get_dependency_tree_depth(simple_sentence),\n",
    "        # self.get_dependency_tree_depth(complex_sentence)),2)\n",
    "        # print(score)\n",
    "        # print('type of score', type(score))\n",
    "        #  score\n",
    "        # return score\n",
    "\n",
    "    def get_dependency_tree_depth(self, sentence):\n",
    "        def get_subtree_depth(node):\n",
    "            if len(list(node.children)) == 0:\n",
    "                return 0\n",
    "            result= 1 + max([get_subtree_depth(child) for child in node.children])\n",
    "            print(type(result))\n",
    "            return result\n",
    "        \n",
    "        tree_depths = [get_subtree_depth(spacy_sentence.root) for spacy_sentence in self.nlp(sentence).sents]\n",
    "        if len(tree_depths) == 0:\n",
    "            return 0\n",
    "        print('type of tree depths', type(max(tree_depths)))\n",
    "        print(max(tree_depths))\n",
    "        print(type(max(tree_depths)))\n",
    "        return max(tree_depths)\n",
    "\n",
    "    # def spacy_process(self, text):\n",
    "    #     return get_spacy_model()(text)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, features_kwargs=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = self.get_features(features_kwargs)\n",
    "        if features_kwargs:\n",
    "            self.hash = source.utils.generate_hash(str(features_kwargs).encode())\n",
    "        else:\n",
    "            self.hash = \"no_feature\"\n",
    "\n",
    "    def get_class(self, class_name, *args, **kwargs):\n",
    "        return globals()[class_name](*args, **kwargs)\n",
    "\n",
    "    def get_features(self, feature_kwargs):\n",
    "        features = []\n",
    "        for feature_name, kwargs in feature_kwargs.items():\n",
    "            features.append(self.get_class(feature_name, **kwargs))\n",
    "        return features\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        if self.features:\n",
    "            line = ''\n",
    "            for feature in self.features:\n",
    "                line += feature.encode_sentence(sentence) + ' '\n",
    "            line += ' ' + sentence\n",
    "            return line.rstrip()\n",
    "        else:\n",
    "            return sentence\n",
    "\n",
    "    def encode_sentence_pair(self, complex_sentence, simple_sentence):\n",
    "        # print(complex_sentence)\n",
    "        if self.features:\n",
    "            line = ''\n",
    "            for feature in self.features:\n",
    "                # startTime = timeit.default_timer()\n",
    "                # print(feature)\n",
    "                processed_complex, _ = feature.encode_sentence_pair(complex_sentence, simple_sentence)\n",
    "                line += processed_complex + ' '\n",
    "                # print('featured sentence', line)\n",
    "                # print(feature, timeit.default_timer() - startTime)\n",
    "            line += ' ' + complex_sentence\n",
    "            return line.rstrip()\n",
    "\n",
    "        else:\n",
    "            return complex_sentence\n",
    "\n",
    "    def decode_sentence(self, encoded_sentence):\n",
    "        for feature in self.features:\n",
    "            decoded_sentence = feature.decode_sentence(encoded_sentence)\n",
    "        return decoded_sentence\n",
    "\n",
    "    def encode_file(self, input_filepath, output_filepath):\n",
    "        with open(output_filepath, 'w') as f:\n",
    "            for line in source.utils.yield_lines(input_filepath):\n",
    "                f.write(self.encode_sentence(line) + '\\n')\n",
    "\n",
    "    def decode_file(self, input_filepath, output_filepath):\n",
    "        with open(output_filepath, 'w') as f:\n",
    "            for line in source.utils.yield_lines(input_filepath):\n",
    "                f.write(self.decode_sentence(line) + '\\n')\n",
    "\n",
    "    def process_encode_sentence_pair(self, sentences):\n",
    "        # print(f\"{sentences[2]}/{self.line_count}\", sentences[0])  # sentence[0] index\n",
    "        return (self.encode_sentence_pair(sentences[0], sentences[1]))\n",
    "\n",
    "    def pool_encode_sentence_pair(self, args):\n",
    "        # print(f\"{processed_line_count}/{self.line_count}\")\n",
    "        complex_sent, simple_sent, queue = args\n",
    "        queue.put(1)\n",
    "        return self.encode_sentence_pair(complex_sent, simple_sent)\n",
    "\n",
    "    @print_execution_time\n",
    "    def encode_file_pair(self, complex_filepath, simple_filepath):\n",
    "        # print(f\"Preprocessing file: {complex_filepath}\")\n",
    "        processed_complex_sentences = []\n",
    "        self.line_count = source.utils.count_line(simple_filepath)\n",
    "\n",
    "        nb_cores = multiprocessing.cpu_count()\n",
    "        manager = multiprocessing.Manager()\n",
    "        queue = manager.Queue()\n",
    "\n",
    "        pool = Pool(processes=nb_cores)\n",
    "        args = [(complex_sent, simple_sent, queue) for complex_sent, simple_sent in\n",
    "                source.utils.yield_sentence_pair(complex_filepath, simple_filepath)]\n",
    "        res = pool.map_async(self.pool_encode_sentence_pair, args)\n",
    "        while not res.ready():\n",
    "            # remaining = res._number_left * res._chunksize\n",
    "            size = queue.qsize()\n",
    "            # print(f\"Preprocessing: {size} / {self.line_count}\")\n",
    "            time.sleep(0.5)\n",
    "        encoded_sentences = res.get()\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        # pool.terminate()\n",
    "        # i = 0\n",
    "        # for complex_sentence, simple_sentence in yield_sentence_pair(complex_filepath, simple_filepath):\n",
    "        # # print(complex_sentence)\n",
    "        #     processed_complex_sentence = self.encode_sentence_pair(complex_sentence, simple_sentence)\n",
    "        #     i +=1\n",
    "        #     print(f\"{i}/{self.line_count}\", processed_complex_sentence)\n",
    "        # processed_complex_sentences.append(encoded_complex)\n",
    "\n",
    "        return encoded_sentences\n",
    "\n",
    "    def get_preprocessed_filepath(self, dataset, phase, type):\n",
    "        filename = f'{dataset}.{phase}.{type}'\n",
    "        return self.preprocessed_data_dir / filename\n",
    "\n",
    "    def preprocess_dataset(self, dataset):\n",
    "        # download_requirements()\n",
    "        # print('self.hash', self.hash)\n",
    "        # print('dataset', dataset)\n",
    "        self.preprocessed_data_dir = PROCESSED_DATA_DIR /  dataset #self.hash /\n",
    "        self.preprocessed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        source.utils.save_preprocessor(self)\n",
    "        # print(f'Preprocessing dataset: {dataset}')\n",
    "\n",
    "        for phase in PHASES:\n",
    "            # for phase in [\"train\", \"valid\"]: \n",
    "            complex_filepath = get_data_filepath(dataset, phase, 'orig')\n",
    "            simple_filepath = get_data_filepath(dataset, phase, 'simp')\n",
    "\n",
    "            complex_output_filepath = self.preprocessed_data_dir / complex_filepath.name\n",
    "            simple_output_filepath = self.preprocessed_data_dir / simple_filepath.name\n",
    "            if complex_output_filepath.exists() and simple_output_filepath.exists():\n",
    "                continue\n",
    "\n",
    "            # print(f'Prepocessing files: {complex_filepath.name} {simple_filepath.name}')\n",
    "            processed_complex_sentences = self.encode_file_pair(complex_filepath, simple_filepath)\n",
    "\n",
    "            source.utils.write_lines(processed_complex_sentences, complex_output_filepath)\n",
    "            shutil.copy(simple_filepath, simple_output_filepath)\n",
    "\n",
    "        # print(f'Preprocessing dataset \"{dataset}\" is finished.')\n",
    "        return self.preprocessed_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    features_kwargs = {\n",
    "        'CharLengthRatioFeature': {'target_ratio': 0.8},\n",
    "        'WordLengthRatioFeature': {'target_ratio': 0.8},\n",
    "        'LevenshteinRatioFeature': {'target_ratio': 0.8},\n",
    "        'WordRankRatioFeature': {'target_ratio': 0.8},\n",
    "        'DependencyTreeDepthRatioFeature': {'target_ratio': 0.8}\n",
    "        }\n",
    "    # features_kwargs = {}\n",
    "    preprocessor = Preprocessor(features_kwargs)\n",
    "    # preprocessor.preprocess_dataset(ASSET_DATASET)\n",
    "    preprocessor.preprocess_dataset(WIKILARGE_DATASET)\n",
    "    # preprocessor.preprocess_dataset(NEWSELA_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_pair = {\"original_text\": \"Dit is de complexe versie en meer contextuele informatie.\", \n",
    "#                   \"simple_text\": \"Deze is de makkelijke versie.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_text = \"Een signaaltransductie in de biologie is een cellulair mechanisme.\"\n",
    "original_text= \"Veel ziekteprocessen, zoals diabetes, hartaandoeningen, auto-immuniteit en kanker, ontstaan door signaaltransductieroutes, wat het signaaltransductie geneeskunde.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Een',\n",
       " 'signaaltransductie',\n",
       " 'in',\n",
       " 'de',\n",
       " 'biologie',\n",
       " 'is',\n",
       " 'een',\n",
       " 'cellulair',\n",
       " 'mechanisme',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.utils.tokenize(simple_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Veel',\n",
       " 'ziekteprocessen',\n",
       " ',',\n",
       " 'zoals',\n",
       " 'diabetes',\n",
       " ',',\n",
       " 'hartaandoeningen',\n",
       " ',',\n",
       " 'auto-immuniteit',\n",
       " 'en',\n",
       " 'kanker',\n",
       " ',',\n",
       " 'ontstaan',\n",
       " 'door',\n",
       " 'signaaltransductieroutes',\n",
       " ',',\n",
       " 'wat',\n",
       " 'het',\n",
       " 'signaaltransductie',\n",
       " 'geneeskunde',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.utils.tokenize(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'W_0.48'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_length = WordLengthRatioFeature(0.8)    # CHECK!!!\n",
    "sentences_pair = word_length.encode_sentence_pair(original_text, simple_text)\n",
    "sentences_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C_0.41'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ratio = CharLengthRatioFeature(0.8)\n",
    "sentences_pair = char_ratio.encode_sentence_pair(original_text, simple_text)\n",
    "sentences_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rank = WordRankRatio2(0.8)\n",
    "sentences_pair = word_rank.encode_sentence_pair_stripped(original_text, simple_text)\n",
    "sentences_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'L_0.25'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein = LevenshteinRatioFeature(0.8)\n",
    "sentences_pair = levenshtein.encode_sentence_pair(original_text, simple_text)\n",
    "sentences_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "type of tree depths <class 'int'>\n",
      "3\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "type of tree depths <class 'int'>\n",
      "4\n",
      "<class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DTD_0.75'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency= DependencyTreeDepthRatioFeature(0.8)\n",
    "dependency.encode_sentence_pair(original_text, simple_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKILARGE_DATASET_PATH = DATASETS_PATH / \"wikilarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_text = pd.read_csv(WIKILARGE_DATASET_PATH / \"wikilarge.train.orig\", header=None, sep=\"\\t\", names=[\"complex_text\"], nrows=20)\n",
    "simple_text = pd.read_csv(WIKILARGE_DATASET_PATH / \"wikilarge.train.simp\", header=None, sep=\"\\t\",names=[\"simple_text\"], nrows=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_pairs = pd.concat([complex_text, simple_text], axis=1)\n",
    "sentences_pairs=sentences_pairs[0:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i,row in complex_text.iterrows():\n",
    "    CLR = char_ratio.encode_sentence_pair_stripped(complex_text, simple_text)\n",
    "    WR= word_length.encode_sentence_pair_stripped(complex_text, simple_text)\n",
    "    LR= levenshtein.encode_sentence_pair_stripped(complex_text, simple_text)\n",
    "    WRR = word_rank.encode_sentence_pair_stripped(complex_text, simple_text)\n",
    "    DTDR = dependency.encode_sentence_pair_stripped(complex_text, simple_text)\n",
    "    results.append((CLR, DTDR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
